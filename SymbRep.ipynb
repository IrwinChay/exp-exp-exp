{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "36e0e9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T20:44:01.279522Z",
     "start_time": "2023-11-08T20:44:01.276395Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa7b2fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T19:08:39.701534Z",
     "start_time": "2023-11-08T19:08:39.682466Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Iterable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d087663e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T16:03:42.464123Z",
     "start_time": "2023-11-08T16:03:42.330280Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.region_graph.rg_node import RGNode, RegionNode, PartitionNode\n",
    "from cirkit.region_graph.region_graph import RegionGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef3fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:16:10.967277Z",
     "start_time": "2023-11-09T00:16:10.963512Z"
    }
   },
   "source": [
    "# Symbolic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "08782ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:13:12.046253Z",
     "start_time": "2023-11-08T23:13:12.035178Z"
    }
   },
   "outputs": [],
   "source": [
    "class SymbLayer(ABC): \n",
    "    inputs: List[ABC]\n",
    "    outputs: List[ABC]\n",
    "    scope: Iterable[int]\n",
    "    layer_type: int \n",
    "    weight_placeholder: torch.Tensor\n",
    "    output_placeholder: torch.Tensor\n",
    "    \n",
    "    real_weights_pointer: torch.Tensor\n",
    "    \n",
    "    product_operation: Tuple\n",
    "    \n",
    "    def __init__(self, layer_type, scope, hidden_dim=None, output_dim=None, weight=None): \n",
    "        assert (layer_type in [0,1,2])\n",
    "        self.layer_type = layer_type\n",
    "        self.scope = scope\n",
    "        \n",
    "        if weight is not None: \n",
    "            assert (layer_type == 0)\n",
    "            self.output_placeholder = torch.empty(weight.shape[0]) \n",
    "            self.weight_placeholder = torch.empty((weight.shape[0], weight.shape[1])) \n",
    "            \n",
    "        if hidden_dim is not None: \n",
    "            if layer_type == 0: \n",
    "                assert (output_dim is not None)\n",
    "                self.output_placeholder = torch.empty(output_dim) \n",
    "                self.weight_placeholder = torch.empty((output_dim, hidden_dim**2)) \n",
    "            elif layer_type == 1: \n",
    "                self.output_placeholder = torch.empty(hidden_dim**2)\n",
    "            elif layer_type == 2: \n",
    "                self.output_placeholder = torch.empty(hidden_dim)\n",
    "        \n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "    def set_product_operation(self, product_operation): \n",
    "        self.product_operation = product_operation \n",
    "        \n",
    "    def set_real_weights_pointer(self, real_weights_pointer): \n",
    "        self.real_weights_pointer = real_weights_pointer\n",
    "    \n",
    "    @property \n",
    "    def is_sum(self): \n",
    "        return self.layer_type == 0\n",
    "    @property\n",
    "    def is_product(self): \n",
    "        return self.layer_type == 1\n",
    "    @property\n",
    "    def is_leaf(self): \n",
    "        return self.layer_type == 2\n",
    "    \n",
    "# actually not used \n",
    "class SymbGraph(): \n",
    "    def __init__(self): \n",
    "        self._layers: Set[SymbLayer] = set()\n",
    "            \n",
    "    def add_layer(self, layer: SymbLayer): \n",
    "        self._layers.add(layer)\n",
    "        \n",
    "    def add_edge(self, tail: SymbLayer, head: SymbLayer):\n",
    "        self._layers.add(tail)\n",
    "        self._layers.add(head)\n",
    "        tail.outputs.append(head)\n",
    "        head.inputs.append(tail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23c979",
   "metadata": {},
   "source": [
    "# Build 2 PC in symbolic represenatation, \n",
    "# s_graph with hidden_dim = 3; \n",
    "# s_graph_2 with hidden_dim = 5; \n",
    "# Both with the same structure but different scope ([1,2,3,4] and [3,4,5,6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "5b7c8638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:13:15.895433Z",
     "start_time": "2023-11-08T23:13:15.881993Z"
    }
   },
   "outputs": [],
   "source": [
    "s_graph = SymbGraph()\n",
    "\n",
    "graph_1_hidden_dim = 3\n",
    "\n",
    "n1 = SymbLayer(2, [1], graph_1_hidden_dim)\n",
    "n2 = SymbLayer(2, [2], graph_1_hidden_dim)\n",
    "np1 = SymbLayer(1, [1,2], graph_1_hidden_dim)\n",
    "ns1 = SymbLayer(0, [1,2], graph_1_hidden_dim, graph_1_hidden_dim)\n",
    "\n",
    "n3 = SymbLayer(2, [3], graph_1_hidden_dim)\n",
    "n4 = SymbLayer(2, [4], graph_1_hidden_dim)\n",
    "np2 = SymbLayer(1, [3,4], graph_1_hidden_dim)\n",
    "ns2 = SymbLayer(0, [3,4], graph_1_hidden_dim, graph_1_hidden_dim) \n",
    "\n",
    "np3 = SymbLayer(1, [1,2,3,4], graph_1_hidden_dim)\n",
    "ns3 = SymbLayer(0, [1,2,3,4], graph_1_hidden_dim, 1)\n",
    "\n",
    "s_graph.add_edge(n1, np1)\n",
    "s_graph.add_edge(n2, np1)\n",
    "s_graph.add_edge(np1, ns1)\n",
    "\n",
    "s_graph.add_edge(n3, np2)\n",
    "s_graph.add_edge(n4, np2)\n",
    "s_graph.add_edge(np2, ns2) \n",
    "\n",
    "s_graph.add_edge(ns1, np3)\n",
    "s_graph.add_edge(ns2, np3)\n",
    "s_graph.add_edge(np3, ns3) \n",
    "\n",
    "# PC2 \n",
    "\n",
    "s_graph_2 = SymbGraph()\n",
    "\n",
    "graph_2_hidden_dim = 5\n",
    "\n",
    "n1_2 = SymbLayer(2, [5], graph_2_hidden_dim)\n",
    "n2_2 = SymbLayer(2, [6], graph_2_hidden_dim)\n",
    "np1_2 = SymbLayer(1, [5,6], graph_2_hidden_dim)\n",
    "ns1_2 = SymbLayer(0, [5,6], graph_2_hidden_dim, graph_2_hidden_dim)\n",
    "\n",
    "n3_2 = SymbLayer(2, [3], graph_2_hidden_dim)\n",
    "n4_2 = SymbLayer(2, [4], graph_2_hidden_dim)\n",
    "np2_2 = SymbLayer(1, [3,4], graph_2_hidden_dim)\n",
    "ns2_2 = SymbLayer(0, [3,4], graph_2_hidden_dim, graph_2_hidden_dim) \n",
    "\n",
    "np3_2 = SymbLayer(1, [3,4,5,6], graph_2_hidden_dim)\n",
    "ns3_2 = SymbLayer(0, [3,4,5,6], graph_2_hidden_dim, 1)\n",
    "\n",
    "s_graph_2.add_edge(n1_2, np1_2)\n",
    "s_graph_2.add_edge(n2_2, np1_2)\n",
    "s_graph_2.add_edge(np1_2, ns1_2)\n",
    "\n",
    "s_graph_2.add_edge(n3_2, np2_2)\n",
    "s_graph_2.add_edge(n4_2, np2_2)\n",
    "s_graph_2.add_edge(np2_2, ns2_2) \n",
    "\n",
    "s_graph_2.add_edge(ns1_2, np3_2)\n",
    "s_graph_2.add_edge(ns2_2, np3_2)\n",
    "s_graph_2.add_edge(np3_2, ns3_2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc9cd0",
   "metadata": {},
   "source": [
    "# set parameters of the two folded PCs, \n",
    "# First PC: Q_1 and WV_1; second PC: Q_2 and WV_2\n",
    "# Then in each sum node of the unfolded symbolic representation, points to the corresponding slice of the folded parammeters\n",
    "\n",
    "# this is to emulate [folded tensorized circuit -> symbolic representation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "cde9ae74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:18:17.220520Z",
     "start_time": "2023-11-08T23:18:17.213644Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.reparams.leaf import ReparamExp\n",
    "reparam = ReparamExp\n",
    "# num_folds, output_dim, input_dim \n",
    "Q_1 = reparam((1, 1, graph_1_hidden_dim**2), dim=2)\n",
    "WV_1 = reparam((2, graph_1_hidden_dim, graph_1_hidden_dim**2), dim=2)\n",
    "\n",
    "Q_2 = reparam((1, 1, graph_2_hidden_dim**2), dim=2)\n",
    "WV_2 = reparam((2, graph_2_hidden_dim, graph_2_hidden_dim**2), dim=2)\n",
    "\n",
    "ns3.set_real_weights_pointer(Q_1.param[0])\n",
    "ns1.set_real_weights_pointer(WV_1.param[0])\n",
    "ns2.set_real_weights_pointer(WV_1.param[1])\n",
    "\n",
    "ns3_2.set_real_weights_pointer(Q_2.param[0])\n",
    "ns1_2.set_real_weights_pointer(WV_2.param[0])\n",
    "ns2_2.set_real_weights_pointer(WV_2.param[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b549e3",
   "metadata": {},
   "source": [
    "# Do the product with symbolic representation, following the pseudocode \n",
    "# but ignoring mixing layer, or different layer types (between layer1 and layer2)\n",
    "\n",
    "# considered sub-PC with distinct scope here, used function match_dim to pad the sub-PC with distinct scope to the dimension of circuit product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "95b0dcf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:24:28.499442Z",
     "start_time": "2023-11-09T00:24:28.468309Z"
    }
   },
   "outputs": [],
   "source": [
    "def symbolic_product(layer1, layer2): \n",
    "    \n",
    "    M = layer1.output_placeholder.shape[0]\n",
    "    N = layer2.output_placeholder.shape[0]\n",
    "    \n",
    "    if not set(layer1.scope).intersection(layer2.scope): \n",
    "        assert (layer1.is_sum or layer1.is_leaf) \n",
    "        assert (layer2.is_sum or layer2.is_leaf)  \n",
    "        \n",
    "        new_layer_1 = match_dim(layer1, N)\n",
    "        new_layer_2 = match_dim(layer2, M)\n",
    "        \n",
    "        new_scope = list(set(new_layer_1.scope) | set(new_layer_2.scope))\n",
    "        \n",
    "        new_product_layer = SymbLayer(1, new_scope, hidden_dim=M*N)\n",
    "        \n",
    "        product_operation = ('diff_scope_product', layer1, layer2)\n",
    "        new_product_layer.set_product_operation(product_operation)\n",
    "\n",
    "        new_layer_1.outputs.append(new_product_layer)\n",
    "        new_layer_2.outputs.append(new_product_layer)\n",
    "        new_product_layer.inputs.append(new_layer_1)\n",
    "        new_product_layer.inputs.append(new_layer_2)\n",
    "        \n",
    "        pseudo_sum_layer = SymbLayer(0, new_scope, hidden_dim=M*N, output_dim=M*N) \n",
    "        \n",
    "        product_operation = ('diff_scope_identity_sum', layer1, layer2)\n",
    "        pseudo_sum_layer.set_product_operation(product_operation)\n",
    "        \n",
    "        new_product_layer.outputs.append(pseudo_sum_layer)\n",
    "        pseudo_sum_layer.inputs.append(new_product_layer)\n",
    "        \n",
    "        return pseudo_sum_layer \n",
    "        \n",
    "        \n",
    "        \n",
    "        # product, then \"identity sum\", but also tweak the vector length (zero padding) \n",
    "    \n",
    "    elif layer1.is_leaf and layer2.is_leaf: \n",
    "        assert (layer1.scope == layer2.scope)\n",
    "        \n",
    "        new_layer = SymbLayer(2, layer1.scope, M*N) \n",
    "        product_operation = ('input_product', layer1, layer2)\n",
    "        new_layer.set_product_operation(product_operation)\n",
    "        return new_layer\n",
    "        \n",
    "    elif layer1.is_sum and layer2.is_sum: \n",
    "        layer1_input = layer1.inputs \n",
    "        layer2_input = layer2.inputs \n",
    "        \n",
    "        assert (len(layer1_input) == 1)\n",
    "        assert (len(layer2_input) == 1)\n",
    "        \n",
    "        new_layer_input = symbolic_product(layer1_input[0], layer2_input[0])\n",
    "        \n",
    "        layer1_weight_placeholder = layer1.weight_placeholder\n",
    "        layer2_weight_placeholder = layer2.weight_placeholder\n",
    "        \n",
    "        new_layer_weight_placeholder = np.kron(layer1_weight_placeholder, layer2_weight_placeholder) \n",
    "    \n",
    "        new_scope = list(set(layer1.scope) | set(layer2.scope))\n",
    "        \n",
    "        new_layer = SymbLayer(0, new_scope, weight=new_layer_weight_placeholder)\n",
    "        \n",
    "        product_operation = ('sum_product', layer1, layer2)\n",
    "        new_layer.set_product_operation(product_operation)\n",
    "        \n",
    "        new_layer_input.outputs.append(new_layer)\n",
    "        new_layer.inputs.append(new_layer_input)\n",
    "        \n",
    "        return new_layer\n",
    "    \n",
    "    elif layer1.is_product and layer2.is_product: \n",
    "        layer1_input = layer1.inputs \n",
    "        layer2_input = layer2.inputs \n",
    "        \n",
    "        assert (len(layer1_input) == 2)\n",
    "        assert (len(layer2_input) == 2)\n",
    "        \n",
    "        new_layer_input_left = symbolic_product(layer1_input[0], layer2_input[0])\n",
    "        new_layer_input_right = symbolic_product(layer1_input[1], layer2_input[1])\n",
    "        \n",
    "        new_scope = list(set(layer1.scope) | set(layer2.scope)) \n",
    "        \n",
    "        new_layer = SymbLayer(1, new_scope, hidden_dim=int(math.sqrt(M*N))) # hack with sqrt \n",
    "        \n",
    "        product_operation = ('product_product', layer1, layer2)\n",
    "        new_layer.set_product_operation(product_operation)\n",
    "        \n",
    "        new_layer_input_left.outputs.append(new_layer)\n",
    "        new_layer_input_right.outputs.append(new_layer)\n",
    "        new_layer.inputs.append(new_layer_input_left)\n",
    "        new_layer.inputs.append(new_layer_input_right)\n",
    "        \n",
    "        return new_layer \n",
    "        \n",
    "        \n",
    "        \n",
    "def match_dim(layer, N): \n",
    "    \n",
    "    M = layer.output_placeholder.shape[0]\n",
    "    \n",
    "    if layer.is_leaf: \n",
    "        \n",
    "        new_layer = SymbLayer(2, layer.scope, M*N) \n",
    "        product_operation = ('diff_scope_input_pad', layer) \n",
    "        new_layer.set_product_operation(product_operation) \n",
    "        return new_layer\n",
    "   \n",
    "    elif layer.is_sum: \n",
    "        \n",
    "        new_layer_input = match_dim( layer.inputs[0], N )\n",
    "        \n",
    "        new_layer = SymbLayer(0, layer.scope, hidden_dim=M*N, output_dim=M*N) # hack \n",
    "        \n",
    "        product_operation = ('diff_scope_sum_pad', layer)\n",
    "        new_layer.set_product_operation(product_operation)\n",
    "        \n",
    "        new_layer_input.outputs.append(new_layer)\n",
    "        new_layer.inputs.append(new_layer_input)\n",
    "        \n",
    "        return new_layer\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif layer.is_product: \n",
    "        \n",
    "        new_layer_input_left = match_dim( layer.inputs[0], N )\n",
    "        new_layer_input_right = match_dim( layer.inputs[1], N )\n",
    "        \n",
    "        new_layer = SymbLayer(1, layer.scope, hidden_dim=M*N) \n",
    "        \n",
    "        product_operation = ('diff_scope_product', layer)\n",
    "        new_layer.set_product_operation(product_operation)\n",
    "        \n",
    "        new_layer_input_left.outputs.append(new_layer)\n",
    "        new_layer_input_right.outputs.append(new_layer)\n",
    "        new_layer.inputs.append(new_layer_input_left)\n",
    "        new_layer.inputs.append(new_layer_input_right)\n",
    "        \n",
    "        return new_layer\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f514f",
   "metadata": {},
   "source": [
    "# do symbolic product, and feel free to trace into the input of each node to examinate the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "efeaa5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:18:20.813295Z",
     "start_time": "2023-11-08T23:18:20.804119Z"
    }
   },
   "outputs": [],
   "source": [
    "symb_product_result = symbolic_product(ns3, ns3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c95ac7ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:22:09.301196Z",
     "start_time": "2023-11-08T23:22:09.292791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_type': 1,\n",
       " 'scope': [1, 2, 5, 6],\n",
       " 'output_placeholder': tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4223e-05,\n",
       "          4.5850e-41, -5.4223e-05,  4.5850e-41,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.4013e-45,  0.0000e+00,  0.0000e+00,  1.4013e-45,  0.0000e+00,\n",
       "         -2.0000e+00,  4.0208e-32,  1.5849e+29,  7.2727e-43,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00, -4.7265e-05,  4.5850e-41,\n",
       "         -4.7265e-05,  4.5850e-41,  0.0000e+00,  0.0000e+00,  4.0210e-32,\n",
       "         -1.5849e+29,  4.0208e-32,  1.5849e+29,  7.2027e-43,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -4.7253e-05,  4.5850e-41, -4.7253e-05,  4.5850e-41,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]),\n",
       " 'inputs': [<__main__.SymbLayer at 0x7fd0b8613fa0>,\n",
       "  <__main__.SymbLayer at 0x7fd0b7006a00>],\n",
       " 'outputs': [<__main__.SymbLayer at 0x7fd0b7006220>],\n",
       " 'product_operation': ('diff_scope_product',\n",
       "  <__main__.SymbLayer at 0x7fd0b88c0cd0>,\n",
       "  <__main__.SymbLayer at 0x7fd0b88c0fd0>)}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symb_product_result.__dict__['inputs'][0].__dict__['inputs'][0].__dict__\n",
    "symb_product_result.__dict__['inputs'][0].__dict__['inputs'][1].__dict__\n",
    "# symb_product_result.__dict__['inputs'][0].__dict__['inputs']\n",
    "\n",
    "# HAVE PARAMS\n",
    "# symb_product_result.__dict__['inputs'][0].__dict__['inputs'][1].__dict__['product_operation'][1].__dict__\n",
    "# symb_product_result.__dict__['inputs'][0].__dict__['inputs'][1].__dict__['product_operation'][2].__dict__\n",
    "\n",
    "symb_product_result.__dict__['inputs'][0].__dict__['inputs'][0].__dict__['inputs'][0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321dfbc",
   "metadata": {},
   "source": [
    "# This part is to emulate the step to build folded tensorized PC from symbolic representation of a circuit product \n",
    "\n",
    "# got topological_layer_result after topological sort \n",
    "\n",
    "# it is not using the parameters in TuckerLayer(). Instead, it is using params. \n",
    "\n",
    "# By constructing the parameters in each fold and concatenating (appending) each fold \n",
    "\n",
    "# Condition: if the current fold is \"product_operation\", or \"diff_scope_identity_sum\", or \"diff_scope_sum_pad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "088a43d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:08:41.608291Z",
     "start_time": "2023-11-09T00:08:41.583990Z"
    }
   },
   "outputs": [],
   "source": [
    "from cirkit.layers.sum_product.tucker import TuckerLayer\n",
    "\n",
    "# AFTER topological sort, \n",
    "\n",
    "symb_sum_1 = symb_product_result.__dict__['inputs'][0].__dict__['inputs'][0]\n",
    "symb_product_1 = symb_sum_1.__dict__['inputs'][0]\n",
    "\n",
    "symb_sum_2 = symb_product_result.__dict__['inputs'][0].__dict__['inputs'][1]\n",
    "symb_product_2 = symb_sum_2.__dict__['inputs'][0]\n",
    "\n",
    "topological_layer_result = [(symb_product_1, symb_product_2), (symb_sum_1, symb_sum_2)]\n",
    "\n",
    "shape_of_input_to_product = topological_layer_result[0][1].__dict__['inputs'][0].__dict__['output_placeholder'].shape[0]\n",
    "shape_of_output = topological_layer_result[1][0].__dict__['output_placeholder'].shape[0]\n",
    "\n",
    "layer = TuckerLayer(shape_of_input_to_product, shape_of_output, arity=2, num_folds=len(topological_layer_result[0]))\n",
    "\n",
    "params = [] \n",
    "\n",
    "for symb_sum in topological_layer_result[1]: \n",
    "    product_operation = symb_sum.__dict__['product_operation']\n",
    "    if product_operation[0] == 'sum_product': \n",
    "        param_product_1 = product_operation[1].__dict__['real_weights_pointer'] \n",
    "        param_product_2 = product_operation[2].__dict__['real_weights_pointer'] \n",
    "        params.append(torch.kron(param_product_1, param_product_2)  ) \n",
    "        \n",
    "    elif product_operation[0] == 'diff_scope_identity_sum': \n",
    "        rows = symb_sum.__dict__['weight_placeholder'].shape[0]\n",
    "        cols = symb_sum.__dict__['weight_placeholder'].shape[1] \n",
    "        \n",
    "        # set gradient of this particular fold to 0 during gradient update \n",
    "        identity_sum = np.hstack((np.eye(rows), np.zeros((rows, cols - rows)))) # actually need to preturb, not just identity + zeros\n",
    "        params.append(identity_sum) \n",
    "        \n",
    "    elif product_operation[0] == 'diff_scope_sum_pad': \n",
    "        weight_original = product_operation[1].__dict__['real_weights_pointer']\n",
    "        \n",
    "        shape_product = symb_sum['weight_placeholder'].shape\n",
    "        shape_original = weight_original.shape\n",
    "        \n",
    "        shape_other = shape_product[0]/shape_original[0] # hack\n",
    "        \n",
    "        # set gradient of this particular component to 0 during gradient update \n",
    "        padding_matrix = np.concatenate( ( np.ones ((1,shape_other**2)) , np.zeros ( (shape_other-1,shape_other**2)) ) , axis=0)\n",
    "        \n",
    "        padded_sum_weight = torch.kron(padding_matrix, weight_original)\n",
    "        params.append(padded_sum_weight) \n",
    "        \n",
    "        \n",
    "params = torch.tensor(params)    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "8a8f0c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:10:24.641483Z",
     "start_time": "2023-11-09T00:10:24.628616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., -0., 0.,  ..., 0., -0., 0.],\n",
       "         [-0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., 0.,  ..., 0., -0., 0.],\n",
       "         ...,\n",
       "         [0., -0., 0.,  ..., 0., -0., 0.],\n",
       "         [-0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [0., -0., 0.,  ..., 0., -0., 0.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230cd7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T21:58:15.073600Z",
     "start_time": "2023-11-08T21:58:15.052970Z"
    }
   },
   "source": [
    "# pad the sub-PC with distinct scope: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "fd27b3f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:45:48.480274Z",
     "start_time": "2023-11-09T00:45:48.475837Z"
    }
   },
   "outputs": [],
   "source": [
    "M=2\n",
    "N=3 \n",
    "\n",
    "leaf_left = np.random.rand(2)\n",
    "leaf_right = np.random.rand(2)\n",
    "weights_original = np.random.rand(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "13ff382b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:48:23.992285Z",
     "start_time": "2023-11-09T00:48:23.977071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91967042, 0.78356915, 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_left_pad = np.pad(leaf_left, ((0, M*N-M)), 'constant')\n",
    "leaf_right_pad = np.pad(leaf_right, ((0, M*N-M)), 'constant')\n",
    "\n",
    "identity_pad = np.concatenate( ( np.ones((1,N**2)), np.zeros((N-1,N**2)) ) , axis=0)\n",
    "\n",
    "weights_pad = np.kron(identity_pad, weights_original)\n",
    "                    # first weight dim = M*(M^2), second weight dim = N*(N^2)\n",
    "result = np.matmul(weights_pad, np.kron(leaf_left_pad, leaf_right_pad) ) \n",
    "\n",
    "original_result = np.matmul(weights_original, np.kron(leaf_left, leaf_right))\n",
    "\n",
    "assert (np.linalg.norm(result[:2] - original_result) < 1e-7)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a3c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6a487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c28641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2fd6f003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T22:03:30.943040Z",
     "start_time": "2023-11-08T22:03:30.929106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_type': 2,\n",
       " 'scope': [1],\n",
       " 'output_placeholder': tensor([-9.1859e-11,  4.5850e-41, -9.5739e-13,  4.5850e-41, -3.7259e-10,\n",
       "          4.5850e-41, -3.4776e-10,  4.5850e-41, -7.9532e-11,  4.5850e-41,\n",
       "         -9.5709e-13,  4.5850e-41, -3.7247e-10,  4.5850e-41, -3.7259e-10]),\n",
       " 'inputs': [],\n",
       " 'outputs': [<__main__.SymbLayer at 0x7fd0b8895520>],\n",
       " 'product_operation': ('diff_scope_input_pad',\n",
       "  <__main__.SymbLayer at 0x7fd0b86be2b0>)}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_test_1 = SymbLayer(2, [1], 3)\n",
    "# n_test_2 = SymbLayer(2, [2], 5)\n",
    "\n",
    "# np_test_1 = SymbLayer(1, [1,2], 3)\n",
    "# ns_test_1 = SymbLayer(0, [1,2], 3, 3)\n",
    "\n",
    "# n_test_1.outputs.append(np_test_1)\n",
    "# n_test_2.outputs.append(np_test_1)\n",
    "# np_test_1.inputs.append(n_test_1)\n",
    "# np_test_1.inputs.append(n_test_2)\n",
    "\n",
    "# np_test_1.outputs.append(ns_test_1)\n",
    "# ns_test_1.inputs.append(np_test_1)\n",
    "\n",
    "# n_test_3 = SymbLayer(2, [3], 5)\n",
    "# n_test_4 = SymbLayer(2, [4], 5)\n",
    "\n",
    "# np_test_2 = SymbLayer(1, [3,4], 5)\n",
    "\n",
    "# ns_test_2 = SymbLayer(0, [3,4], 5, 5)\n",
    "\n",
    "# n_test_3.outputs.append(np_test_2)\n",
    "# n_test_4.outputs.append(np_test_2)\n",
    "# np_test_2.inputs.append(n_test_3)\n",
    "# np_test_2.inputs.append(n_test_4)\n",
    "\n",
    "# np_test_2.outputs.append(ns_test_2)\n",
    "# ns_test_2.inputs.append(np_test_2)\n",
    "\n",
    "\n",
    "# symbolic_product(ns_test_1, ns_test_2).__dict__['inputs'][0].__dict__['inputs'][0].__dict__['inputs'][0].__dict__['inputs'][0].__dict__\n",
    "\n",
    "\n",
    "# # .__dict__['inputs'][0].__dict__['inputs'][1].__dict__['product_operation'][-1].__dict__ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf3a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830c265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fb6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177bed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1750d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T16:25:02.075613Z",
     "start_time": "2023-11-08T16:25:02.067621Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30a32179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T16:27:29.445667Z",
     "start_time": "2023-11-08T16:27:29.424158Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3382844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cirkit1",
   "language": "python",
   "name": "cirkit1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
